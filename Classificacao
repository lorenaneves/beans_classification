!pip install feature_engine
!pip install xgboost
!pip install shap

from feature_engine.encoding import OneHotEncoder
from feature_engine.imputation import ArbitraryNumberImputer
from feature_engine.imputation import CategoricalImputer
from feature_engine.selection import DropConstantFeatures
from feature_engine.selection import DropDuplicateFeatures
from feature_engine.selection import SmartCorrelatedSelection
from feature_engine.selection import SelectByShuffling
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import KFold, cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score
import pandas as pd
import numpy as np
import gc
from sklearn.metrics import roc_curve, auc
import gc
gc.collect()
import sklearn.model_selection as model_selection
from sklearn.model_selection import train_test_split
import matplotlib.cm as cm
from matplotlib.colors import ListedColormap, BoundaryNorm
import matplotlib.patches as mpatches
from matplotlib import cm
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

### Download_Da_Base
data_xls = pd.read_excel('Dry_Bean_Dataset.xlsx')

### Converte_Para_CSV
data_xls.to_csv('csvfile.csv')

##Entender_os_Dados

print(data_xls.shape)
#13611 graos
##17 features

#Classificacoes_Possiveis
(data_xls['Class'].unique())
#['SEKER' 'BARBUNYA' 'BOMBAY' 'CALI' 'HOROZ' 'SIRA' 'DERMASON']

#Classificacoes_Distribuicao
print(data_xls.groupby('Class').size())

#Class
#BARBUNYA    1322
#BOMBAY       522
#CALI        1630
#DERMASON    3546
#HOROZ       1928
#SEKER       2027
#SIRA        2636
#dtype: int64

import seaborn as sns
import matplotlib.pyplot as plt
sns.countplot(data_xls['Class'],label="Count")
plt.show()

#Transformar_Categoricas_em_Numericas
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
x = data_xls['Class']
y = label_encoder.fit_transform(x)
class_cat = label_encoder.fit_transform(x)
print(y)

variaveis = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRation', 'Eccentricity', 'ConvexArea', 
             'EquivDiameter', 'Extent', 'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2',
             'ShapeFactor3', 'ShapeFactor4']
X = data_xls[variaveis]
print(X)

#Train_VS_Test
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#Support_Vector_Machine
svm = SVC()
svm.fit(X_train, y_train)
print('Accuracy of SVM classifier on training set: {:.2f}'
     .format(svm.score(X_train, y_train)))
print('Accuracy of SVM classifier on test set: {:.2f}'
     .format(svm.score(X_test, y_test)))
     
#Gaussian_Naive_Bayes
gnb = GaussianNB()
gnb.fit(X_train, y_train)
print('Accuracy of GNB classifier on training set: {:.2f}'
     .format(gnb.score(X_train, y_train)))
print('Accuracy of GNB classifier on test set: {:.2f}'
     .format(gnb.score(X_test, y_test)))
     
#Gaussian_Naive_Bayes
gnb = GaussianNB()
gnb.fit(X_train, y_train)
print('Accuracy of GNB classifier on training set: {:.2f}'
     .format(gnb.score(X_train, y_train)))
print('Accuracy of GNB classifier on test set: {:.2f}'
     .format(gnb.score(X_test, y_test)))
     
#Arvore_de_Decisao
dtc = DecisionTreeClassifier().fit(X_train, y_train)
print('Accuracy of Decision Tree classifier on training set: {:.2f}'
     .format(clf.score(X_train, y_train)))
print('Accuracy of Decision Tree classifier on test set: {:.2f}'
     .format(clf.score(X_test, y_test)))
     
#KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
print('Accuracy of K-NN classifier on training set: {:.2f}'
     .format(knn.score(X_train, y_train)))
print('Accuracy of K-NN classifier on test set: {:.2f}'
     .format(knn.score(X_test, y_test)))
     
#Matriz_de_Confusao
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
pred = knn.predict(X_test)
print(confusion_matrix(y_test, pred))
print(classification_report(y_test, pred))

#Plotar_KNN
k_range = range(1, 20)
scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors = k)
    knn.fit(X_train, y_train)
    scores.append(knn.score(X_test, y_test))
plt.figure()
plt.xlabel('k')
plt.ylabel('acur√°cia')
plt.scatter(k_range, scores)
plt.xticks([0,5,10,15,20])
     
